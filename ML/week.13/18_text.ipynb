{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "___\n",
    "<h1> Machine Learning </h1>\n",
    "<h2> Systems Engineering and Computer Technologies / Engenharia de Sistemas e Tecnologias Informáticas\n",
    "(LESTI)</h2>\n",
    "<h3> Instituto Superior de Engenharia / Universidade do Algarve </h3>\n",
    "\n",
    "[LESTI](https://ise.ualg.pt/curso/1941) / [ISE](https://ise.ualg.pt) / [UAlg](https://www.ualg.pt)\n",
    "\n",
    "Pedro J. S. Cardoso (pcardoso@ualg.pt)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data: an introduction\n",
    "## What is text data?\n",
    "- Text data is a information represented in the form of text. It is a **collection of words, sentences, and paragraphs** that is readable and includes alphabets and numbers.\n",
    " \n",
    "- Text data is everywhere, being one of the most common forms of data that is generated by humans in the form of blogs, tweets, comments, and so on.\n",
    " \n",
    "- With easy speech-to-text services, text data is also generated in the form of audio transcripts, such as call center transcripts, meeting transcripts, and so on.\n",
    "\n",
    "- But, text data is **unstructured data**, and it is not easy to extract information from it.  Text data is **also known as natural language data**.\n",
    "\n",
    "- NLP (Natural Language Processing) is a field of computer science that deals with the interaction between computers and humans using the natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic definitions\n",
    "\n",
    "A text document is a collection of words, sentences, and paragraphs. All in all it not more than a string.\n",
    "\n",
    "#### Corpus\n",
    " The **corpus** is a (large and) **structured collection of texts or written materials** that are used for linguistic analysis, research, or language modeling purposes.\n",
    "\n",
    "#### Lexicon\n",
    "The **lexicon** of a language is the set of all words in that language. It is also called the **vocabulary** of the language.\n",
    "\n",
    "**Words** are also called **terms** and, in some contexts, **tokens**.\n",
    "\n",
    "#### $n$-grams\n",
    "\n",
    "$n$-grams are contiguous sequences of $n$ items from a given sample of text or speech. In the context of natural language processing, an $n$-gram typically refers to a sequence of $n$ words or characters.\n",
    "\n",
    "For example, let's consider the sentence: \"I love to code.\" Here are some examples of $n$-grams with different values of $n$:\n",
    "\n",
    "- Unigrams ($n = 1$): [\"I\", \"love\", \"to\", \"code\"]\n",
    "- Bigrams ($n = 2$): [\"I love\", \"love to\", \"to code\"]\n",
    "- Trigrams ($n = 3$): [\"I love to\", \"love to code\"]\n",
    "- 4-grams ($n = 4$): [\"I love to code\"]\n",
    "\n",
    "The longer the $n$-gram (the higher the value of $n$), the more context you have to work with. In general, a larger $n$-gram generally means more context, which means a better understanding of the structure and sentiment of a text. The optimal value of $n$ depends on the application and the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Example\n",
    "Example, given the list of documents:\n",
    "- D1: \"I like to play football\"\n",
    "- D2: \"I hate football\"\n",
    "- D3: \"I like to play tennis\"\n",
    "\n",
    "As a result\n",
    "\n",
    "- The lexicon is: {I, like, to, play, football, hate, tennis}.\n",
    "- The corpus is: {D1, D2, D3}.\n",
    "- The 2-grams / bigrams are: {I like, like to, to play, play football, I hate, hate football, like to, to play, play tennis}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of text data\n",
    "Text data is used in many applications, such as:\n",
    "- **Sentiment analysis**: Sentiment analysis is the process of analyzing the sentiment of a piece of text. It is used to determine whether the sentiment of a piece of text is positive, negative, or neutral. It is used in many applications, such as social media monitoring, brand monitoring, and customer service.\n",
    "- **Text classification**: Text classification is the process of classifying text into different categories. It is used in many applications, such as spam detection, sentiment analysis, and topic classification.\n",
    "- **Text summarization**: Text summarization is the process of summarizing text into a shorter version. It is used in many applications, such as news summarization, document summarization, and email summarization.\n",
    "- **Machine translation**: Machine translation is the process of translating text from one language to another. It is used in many applications, such as language translation, document translation, and website translation.\n",
    "- **Question answering**: Question answering is the process of answering questions. It is used in many applications, such as question answering systems, question answering systems, and question answering systems.\n",
    "- **Information retrieval**: Information retrieval is the process of retrieving information from a collection of documents.\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basic feature extraction techniques\n",
    "### Dataset\n",
    "\n",
    "In this notebook we'll use the **Large Movie Review Dataset v1.0**, retrieved from http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "This dataset contains **movie reviews along with their associated binary sentiment polarity labels**. It is intended to serve as a benchmark for sentiment classification.\n",
    "\n",
    "\n",
    "The core dataset contains **50k reviews** split evenly into **25k train and 25k test sets**. The overall distribution of labels is balanced (25k pos and 25k neg). It is also include an additional 50k unlabeled documents for unsupervised learning, which we will not use in this notebook.\n",
    "\n",
    "In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. Further, the **train and test sets contain a disjoint set of movies**, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.  \n",
    "\n",
    "In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets. In the unsupervised set, reviews of any rating are included and there are an even number of reviews > 5 and <= 5.\n",
    "\n",
    "Please read the README (in the aclImdb_full_dataset.zip) for more details. We will use the file imdb_data_train.zip which is the compiled version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/imdb_data_train.zip')\n",
    "\n",
    "print(\"Dataframe's shape = \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_Note:_\n",
    "- _the `.head()` function is used to return the first 5 rows of the dataframe._\n",
    "- _the `.loc` function is used to access a group of rows and columns by label(s) or a boolean array. The `.iloc` function is used to access a group of rows and columns by integer location(s)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For example, a review is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_id = 42\n",
    "\n",
    "print('--> review:', df.loc[review_id, 'review'])\n",
    "print('--> classification:', df.loc[review_id, 'classification'])\n",
    "print('--> sentiment:', df.loc[review_id, 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This set of reviews builds out our corpus of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Number of words\n",
    "\n",
    "Counting of words in a document is a basic feature extraction technique. For that we can use the `apply()` function to apply the `split()` function to each document in the dataframe. The `split()` function splits a string into a list separated by a delimiter (the default delimiter is a space). The result is a list of words in the document and the number of words is the length of the list. \n",
    "\n",
    "The result of applying the `apply()` function is a series of the number of words in each document. We can then add this series to the dataframe as a new column.\n",
    "\n",
    "_Note: a `lambda` function is an anonymous function that is used to apply the `split()` function to each document in the dataframe. Its syntax is `lambda arguments: expression`. E.g., `lambda x: len(x.split(\" \"))` is a function that takes a string `x` as an argument and returns the number of words in the string `x`. Another example is `lambda x: x**2`, which returns the square of `x`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the number of words in each review\n",
    "df['#words'] = df['review'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# show the first 5 reviews, with the number of words in each review\n",
    "df[['review', '#words']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Number of characters\n",
    "Counting the number of characters in a document is also a basic feature extraction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['#chars'] = df['review'].str.len()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Average word length\n",
    "Average word length is a feature extraction technique that is used to find the average length of all the words in a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words_lens = [len(word) for word in sentence.split()]\n",
    "  return sum(words_lens)/len(words_lens)\n",
    "\n",
    "df['avg_word_len'] = df['review'].apply(lambda x: avg_word(x))\n",
    "\n",
    "df[['review', 'avg_word_len']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Number of Stopwords\n",
    "Stopwords are the words that are most commonly used in a language, such as \"the\", \"a\", \"an\", \"in\", and \"on\". **These words do not add any meaning to a sentence**. Stopwords are removed to reduce the dimensionality of the data and to remove noise from the data. (see https://en.wikipedia.org/wiki/Stop_word)\n",
    "\n",
    "Obviously, stop words are language dependent.\n",
    "- In English, stopwords are, for example: \"the\", \"a\", \"an\", \"in\", \"on\", etc.\n",
    "- In Portuguese, stopwords are, for example: \"o\", \"a\", \"os\", \"as\", \"em\", \"sobre\", etc.\n",
    "- In Spanish, stopwords are, for example: \"el\", \"la\", \"los\", \"las\", \"en\", \"sobre\", etc.\n",
    "- In japanese, stopwords are, for example: \"の\", \"に\", \"は\", \"を\", \"た\", \"が\", etc. (!)\n",
    "\n",
    "We can use the `nltk` library to count the number of stopwords in a document. The `nltk` library is a collection of natural language processing libraries. It is used to perform various natural language processing tasks, such as tokenization, stemming, lemmatization, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if necessary, install the nltk library\n",
    "#!pip install nltk \n",
    "\n",
    "# if necessary, download the stopwords corpus\n",
    "#import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we can use the `apply()` with an anonymous function to count the number of stopwords in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['#stopwords'] = df['review'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n",
    "\n",
    "df[['review', '#stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As an alternative, to the use of the `lambda` function, we can define a function to count the number of stopwords in a document and then use the `apply()` function to apply the function to each document in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_stopwords(doc):\n",
    "    global stop_words\n",
    "    return len([x for x in doc.split() if x in stop_words])\n",
    "\n",
    "df['#stopwords'] = df['review'].apply(lambda x: count_stopwords(x))\n",
    "df[['review', '#stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Personalized stop words list\n",
    "\n",
    "**Using a preexisting collection of stop words may seem convenient, but it often proves inadequate for specific applications**. Take clinical texts, for instance, where words like \"mcg,\" \"dr.,\" and \"patient\" appear frequently in almost every document. In the context of clinical text mining and retrieval, these terms can be considered as potential stop words. Likewise, when dealing with tweets, terms like \"#,\" \"RT,\" and \"@username\" may qualify as potential stop words. Unfortunately, the standard list of language-specific stop words fails to encompass these domain-specific terms.\n",
    "\n",
    "To set our own stop words, e.g, we can use the following rules:\n",
    "1. set the $n$-most frequent terms in the corpus as stop words\n",
    "2. set the $n$-least frequent terms in the corpus as stop words\n",
    "3. set the $n$-least IDF score terms as stop words (see below)\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Number of special characters\n",
    "Special characters include `!`, `@`, `#`, `$`, `%`, etc. We can use the `count()` function to count the number of special characters in a document. This can be useful in detecting spam, which often contains a lot of special characters.\n",
    "\n",
    "For instance, the number of exclamations can be used the level of excitement, surprise, anger etc. in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['#exclamations'] = df['review'].str.count('!')\n",
    "\n",
    "# show the 5 reviews with the most exclamations, i.e., sorted by the number of exclamations\n",
    "df[['review', '#exclamations']].sort_values(by='#exclamations', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The review with more exclamations is:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = df['#exclamations'].idxmax()\n",
    "df.loc[id, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in twitter, special characters are used to tag topics, so counting special characters can be useful in topic detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#topics'] = df['review'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "df[['review', '#topics']].sort_values(by='#topics', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of numerics\n",
    "We can use the `isdigit()` function to count the number of numerics in a document.\n",
    "This can be useful in detecting spam, which often contains a lot of numerics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#numerics'] = df['review'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "df[['review', '#numerics']].sort_values(by='#numerics', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The review with more numerics is:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "id = df['#numerics'].idxmax()\n",
    "df.loc[id, 'review']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of uppercase words\n",
    "Uppercase words can be used to express anger or excitement. We can use the `isupper()` function to count the number of uppercase words in a document.\n",
    "In our case, maybe is not so useful, as we can see in uppercase words list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upper'] = df['review'].apply(lambda x: [x for x in x.split() if x.isupper()])\n",
    "df['#upper'] = df['upper'].apply(lambda x: len(x))\n",
    "\n",
    "df[['review', 'upper', '#upper']].sort_values(by='#upper', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And the review with more uppercase words is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = df['#upper'].idxmax()\n",
    "df.loc[id, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Pre-processing of text data\n",
    "So far, we have seen how to extract basic features from text data. Now, we will see how to pre-process text data before extracting features from it.\n",
    "\n",
    "Here, **pre-processing refers to the transformations applied to our data before feeding it to some algorithm**. In the context of text data, it is also known as **text cleaning and pre-processing**.\n",
    "\n",
    "So, text normalization, also known as text standardization, is a process that **transforms text into a consistent or canonical form**. Its purpose is to **ensure uniformity and facilitate text processing and analysis**. The normalization process is not a one-size-fits-all approach and can involve various techniques.\n",
    "\n",
    "For example, one common step in normalization is converting all text to lowercase. This straightforward and widely applicable method is effective for text pre-processing. Additionally, dealing with misspelled words, acronyms, short forms, and out-of-vocabulary terms is another approach. For instance, terms like \"super,\" \"superb,\" and \"superrrr\" can be normalized to \"super.\" By applying text normalization techniques, the noise and disruptions in the text data are handled, resulting in cleaner, more reliable data.\n",
    "\n",
    "__Stemming__ and __lemmatization_ are also employed as part of text normalization:\n",
    "- __Stemming__ reduces words to their base or root form.\n",
    "- __lemmatization__ aims to bring words to their canonical or dictionary form. \n",
    "\n",
    "These techniques further contribute to word normalization in text processing.\n",
    "\n",
    "**All in all, the texts are transformed from a sequencial list of words to a multidimensional vector of numbers, as we will see below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $n$-grams\n",
    "Remember, an $n$-gram is a contiguous sequence of $n$ items from a given sample of text or speech.\n",
    " \n",
    "We can use the `ngrams()` function from the `nltk.util` module to generate $n$-grams from a sequence of tokens. The `ngrams()` function takes in two arguments: the sequence of tokens and the value of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the n-grams for the reviews\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# build the n-grams for the reviews\n",
    "df['review_ngrams'] = df['review'].apply(lambda x: list(ngrams(x.split(), 2)))\n",
    "\n",
    "df[['review', 'review_ngrams']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW)\n",
    "A **bag-of-words (BoW) is a representation of text that describes the occurrence of words within a document**. It keeps track of word counts and disregards the grammatical details and the word order.\n",
    "\n",
    "The assumption here is that the higher the count of a word in a document, the more important it is and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `CountVectorizer()` function from the `sklearn.feature_extraction.text` module to perform CountVectorization (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
    "\n",
    "The `CountVectorizer()` function takes in a list of strings and converts it to a matrix of integers. Each row in the matrix represents a document and each column represents a word and the corresponding cell represents the count of that word in that document.\n",
    "\n",
    "Further, for this part, we'll restrict the corpus to the first 10 documents in the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the corpus is a list of strings (documents) to analyze\n",
    "corpus = df['review'].head(10)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize the CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observation can be made at once:\n",
    "- the **matrix is sparse**, i.e., most of the cells are zero\n",
    "\n",
    "- the **matrix is not normalized**, i.e., the number of words in each document is not taken into account\n",
    "\n",
    "- the **matrix contains** a lot of words that might not be useful for some analysis, e.g., \"a\", \"about\", \"above\", \"after\", etc. These words are called **stop words** and they need to be removed from the corpus before performing CountVectorization.\n",
    "\n",
    "- Depending on the context, **some words have the same \"meaning\" but are written differently**, e.g., \"actor\" and \"actress\" (should these words be merged?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer with stop words removal\n",
    "We can remove stopwords before performing CountVectorization or ask it to do it, by passing the list of stopwords to the `stop_words` parameter of the `CountVectorizer()` function or a string with the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer with stop words removal and $n$-grams\n",
    "We can extract $n$-grams by passing the value of $n$ to the `ngram_range` parameter of the `CountVectorizer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(\n",
    "    stop_words='english', \n",
    "    ngram_range=(1,2),\n",
    "    lowercase=True, # lowercase the document, by default is already true\n",
    ")\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaningfulness of the results\n",
    "Bag-of-words does not bring in any information on the meaning of the text. For example, if we consider these two sentences\n",
    "\n",
    "- “Text processing is easy but tedious.”\n",
    "- “Text processing is tedious but easy.”\n",
    "\n",
    "a bag-of-words model would create the same vectors for both of them, even though they have different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Text processing is easy but tedious.',\n",
    "    'Text processing is tedious but easy.'\n",
    "]\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This results is obviously different if $n$-grams are used, like in the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer\n",
    "The `HashingVectorizer()` function from the `sklearn.feature_extraction.text` module is another technique that is used to extract features from text (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). It is also known as `Hashing Trick`.\n",
    "\n",
    "In this technique, we simply **apply a hash function to the terms to convert them into numeric values**. The assumption here is that the hash function will assign unique indexes to the terms and hence we will not need to store the vocabulary explicitly. This will help us save memory.\n",
    "\n",
    "Further, it turns a collection of text documents into a `scipy.sparse matrix` holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if `norm=’l1’` or projected on the euclidean unit sphere if `norm=’l2’`.\n",
    "\n",
    "This strategy has several advantages:\n",
    "- it is very **low memory** scalable to large datasets as there is no need to store a vocabulary dictionary in memory.\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters.\n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there is **no state computed during fit**.\n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "- there is **no way to compute the inverse transform** (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "- there can be **collisions**: distinct tokens can be mapped to the same feature index. However, in practice this is **rarely** an issue if `n_features` is large enough (e.g. $2^{18}$ for text classification problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hv = HashingVectorizer(stop_words='english',\n",
    "                       n_features=500, # note: this is a very small vocabulary... in a real case, we would use a much larger vocabulary\n",
    "                    )\n",
    "\n",
    "# fit_transform() creates the vocabulary and returns a term-document matrix\n",
    "X = hv.fit_transform(corpus)\n",
    "\n",
    "print(X.shape)\n",
    "# build a dataframe with the term-document matrix\n",
    "pd.DataFrame(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, 'Shakespeare' is hashed to 401, 'working' is hashed to 409, 'bad' is hashed to 95, ... and 'benfica' is not in the vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hv.transform(['Shakespeare', 'working','bad', 'benfica']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing\n",
    "Another preprocessing technique is to lowercase the term-document matrix. This avoids having multiple copies of the same word just because it was capitalized differently.\n",
    "\n",
    "For instance, converting to lower case, is the default behavior of the `CountVectorizer()` function from the `sklearn.feature_extraction.text` module (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "But it can also be done manually, using the `lower()` function from the `str` module (https://docs.python.org/3/library/stdtypes.html#str.lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lower'] = df['review'].apply(lambda x: x.lower())\n",
    "\n",
    "df[['review', 'review_lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal\n",
    "Ponctuation is not useful for (basic) text analysis. We can remove it using the `replace()` function from the `re` module (https://docs.python.org/3/library/re.html).\n",
    "\n",
    "The regular expression \"[^\\w\\s]\" matches everything that is not (^) a word character (alphanumeric character) or whitespace. then the `re.sub()` function replaces all the matches with empty string. (see https://regexr.com/ for more information about regular expressions and to test them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['review_no_punctuation'] = df['review_lower'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "df[['review', 'review_no_punctuation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal\n",
    "The remove of stopwords is a common preprocessing step in text analysis. In this example, the stopwords are removed using the `stopwords` corpus from the `nltk` module (https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_no_stopwords'] = df['review_no_punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))\n",
    "\n",
    "df[['review', 'review_no_stopwords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent words removal\n",
    "**Sometimes**, we can also remove the most frequent words from the text data. These words **might** not be useful for text analysis as they are very common and do not provide any information about the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the words in the text\n",
    "words_frequence = df['review_no_stopwords'].str.split(expand=True).unstack().value_counts().sort_values(ascending=False)\n",
    "words_frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 10 most frequent words\n",
    "words_to_remove = words_frequence[:10].index.tolist()\n",
    "print('words to remove:', words_to_remove)\n",
    "\n",
    "df['review_no_frequent_words'] = df['review_no_stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in words_to_remove]))\n",
    "\n",
    "df[['review_no_stopwords', 'review_no_frequent_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare words removal\n",
    "Similar, we can remove the rare words from the text data. These words might not be useful for text analysis as they are very rare and do not provide any information about the text. I.e., because they’re so rare, the association between them and other words is dominated by noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 10 rarest words\n",
    "words_to_remove = words_frequence[-10:].index.tolist()\n",
    "words_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_no_rare_words'] = df['review_no_stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (words_to_remove)]))\n",
    "\n",
    "df[['review_no_stopwords', 'review_no_rare_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction\n",
    "\n",
    "Text reviews (posts in generl) often contain spelling mistakes. We can use the `TextBlob()` function from the `textblob` module (https://textblob.readthedocs.io/en/dev/) to correct the spelling of the words (Spelling correction is based on Peter Norvig’s “How to Write a Spelling Corrector” as implemented in the pattern library. It is about 70% accurate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# this is a time consuming process so we'll just do it for first 10 reviews\n",
    "df['review_corrected'] = df['review']\n",
    "df.loc[:10, 'review_corrected'] = df.loc[:10, 'review'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "df[['review', 'review_corrected']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of splitting a string into a list of pieces or tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. For instance, the sentence \"The cat is brown\" can be tokenized into the list of tokens ['The', 'cat', 'is', 'brown']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['review_tokenized'] = df['review_corrected'].apply(lambda x: word_tokenize(x))\n",
    "df[['review', 'review_tokenized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (see https://en.wikipedia.org/wiki/Stemming).  It chops off the prefixes and suffixes. For instance, the words 'fishing', and 'fished' stem from the word 'fish'. Stemming is useful in text analysis as it reduces the number of words to analyze.\n",
    "\n",
    "In this example, stemming is done using the `SnowballStemmer()` class from the `nltk` module (https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# create an instance of the SnowballStemmer class\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# define a function that applies the stemming to a list of words\n",
    "def stem_doc(doc):\n",
    "    global stemmer\n",
    "    return ' '.join([stemmer.stem(word) for word in doc.split()])\n",
    "\n",
    "sentence = 'actor actors actress actresses fish fishing fisher fishery fisherman fishable am are is was were best well better good'\n",
    "\n",
    "print('Original sentence:', sentence)\n",
    "print('         Stemming:', stem_doc(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stemming to the reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_stemmed'] = df['review'].apply(lambda x: stem_doc(x))\n",
    "\n",
    "df[['review', 'review_stemmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is the process of grouping together the inflected forms of a word, so they can be analysed as a single item, identified by the word's lemma, or dictionary form (from https://en.wikipedia.org/wiki/Lemmatisation).\n",
    "\n",
    "It is closely related to stemming. The main difference is that lemmatization considers the context of the word while normalization is performed, but stemmer doesn't consider the contextual knowledge of the word.\n",
    "\n",
    "Lemmatization is useful in text analysis as it reduces the number of words to analyze.\n",
    "\n",
    "Lemmatization vs. Stemming: Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
    "\n",
    "[e.g., see https://www.turing.com/kb/stemming-vs-lemmatization-in-python]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('treebank')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define a function that applies the lemmatization to a list of words\n",
    "def lemmatize_doc(doc):\n",
    "    #PoS stands for \"part of speech\", the syntactic type of words, such as nouns, pronouns, adjectives, verbs, adverbs, and prepositions.\n",
    "    for pos in ['a', 's', 'r', 'n', 'v']: # a: adjective, s: adjective satellite, r: adverb, n: noun, v: verb\n",
    "        doc = ' '.join([lemmatizer.lemmatize(word, pos) for word in doc.split()])\n",
    "    return doc\n",
    "\n",
    "print('Original sentence:', sentence)\n",
    "print('         Stemming:', stem_doc(sentence))\n",
    "print('    Lemmatization:', lemmatize_doc(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lemmatized'] = df['review'].apply(lambda x: lemmatize_doc(x))\n",
    "\n",
    "df[['review', 'review_lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatives can be found to the `WordNetLemmatizer()` class from the `nltk` module (https://www.nltk.org/). For instance,\n",
    "- the `spacy` module (https://spacy.io/)\n",
    "- the `textblob` module (https://textblob.readthedocs.io/en/dev/)\n",
    "- the `stanza` module (https://stanfordnlp.github.io/stanza/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary\n",
    "\n",
    "The following function summarizes the preprocessing steps we have seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_IMDB(filename, nrows=None):\n",
    "    \"\"\" load the IMDB data and preprocess it:\n",
    "            - remove html tags\n",
    "            - remove ponctuation\n",
    "            - convert to lower case\n",
    "            - remove stop words\n",
    "            - remove numbers\n",
    "            - remove extra spaces\n",
    "            - replave words with their root form (stem)\n",
    "            - replace words with their lemma\n",
    "        :param dataset: 'train' or 'test'\n",
    "        :param nrows: number of rows to read\n",
    "        :return: df\n",
    "    \"\"\"\n",
    "\n",
    "    # read the data\n",
    "    df = pd.read_csv(filename, nrows=nrows)\n",
    "\n",
    "    # keep a copy of the original review\n",
    "    df['original_review'] = df['review']\n",
    "\n",
    "    # remove the html tags\n",
    "    df['review'] = df['review'].str.replace('<br />', ' ')\n",
    "\n",
    "    # remove the punctuation and '_' characters\n",
    "    df['review'] = df['review'].str.replace('[^\\w\\s]', ' ', regex=True)\n",
    "    df['review'] = df['review'].str.replace('_', ' ', regex=False)\n",
    "\n",
    "    # convert to lower case\n",
    "    df['review'] = df['review'].str.lower()\n",
    "\n",
    "    # remove the stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    # remove the numbers - test on https://regexr.com\n",
    "    df['review'] = df['review'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "    # remove the extra spaces - test on https://regexr.com\n",
    "    df['review'] = df['review'].str.replace(' +', ' ', regex=True)\n",
    "\n",
    "    # replace the words with their root form\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "    # replace the words with their lemma\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Advance Text Processing (Optional)\n",
    "\n",
    "More advanced text processing includes term frequency, inverse document frequency etc. These are covered in the following sections.\n",
    "\n",
    "So, first lets us re-read the data (considerer only the 1000 first reviews) and do some basic text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = load_and_preprocess_IMDB(filename='./data/imdb_data_train.zip', nrows=1000)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "The term frequency (TF) of a word is the frequency of the word (i.e. the number of times it appears) in a document. The term frequency is often divided by the sentence/document length (i.e. the total number of words in the sentence/document) as a way of normalization.\n",
    "\n",
    "So the term $t$ frequency in document $d$ is given by:\n",
    "$$\n",
    "\\mbox{term frequency}_{t,d}\n",
    "    = \\frac{\\mbox{number of times the t-word  appears in the d-document}}{\\mbox{total number of words in the d-document }}\n",
    "    = \\frac{n_{t,d}}{\\sum_w n_{w,d}}\n",
    "$$\n",
    "where $n_{t,d}$ is the number of times the $t$-word appears in the $d$-document.\n",
    "\n",
    "For instance, the term frequency of the word 'fish' in the sentence \"The fish is brown\" is 1/4 = 0.25.\n",
    "\n",
    "The term frequency ranges from 0 to 1. The higher the term frequency, the more important the word is to that document.\n",
    "\n",
    "Alternatives and tf-based solution include::\n",
    "- binary: 0, 1 (exists or not in the document)\n",
    "- raw count (term absolute frequency): $n_{i,d}$\n",
    "- log normalization: $\\log(1+n_{i,d})$\n",
    "- double normalization: $0.5 + 0.5\\frac{n_{i,d}}{\\max_{k \\in d} n_{k,d}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_frequency(doc):\n",
    "    \"\"\" Compute the term frequency of a word in a document\n",
    "    :param doc: the document\n",
    "    :return: the term frequency as a pandas series\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the term absolute frequency by doc (i.e. the number of times the word appears in the document)\n",
    "    count_vect = CountVectorizer()\n",
    "    X = count_vect.fit_transform([doc])\n",
    "\n",
    "    # convert the term absolute frequency to a pandas dataframe\n",
    "    bow = pd.DataFrame(X.toarray(), columns=count_vect.get_feature_names_out())\n",
    "\n",
    "    # compute the term frequency by doc (i.e. the number of times the word appears in the document)\n",
    "    tf = bow.div(bow.sum(axis=1), axis=0)\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the term frequency for each review\n",
    "df['review_tf'] = df['review'].apply(lambda doc: compute_term_frequency(doc).to_dict())\n",
    "\n",
    "df[['review', 'review_tf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) is a weight indicating **how commonly a word is used**.\n",
    "\n",
    "The IDF of a word is the measure of how significant that term is in the whole corpus (i.e. the list of all reviews).\n",
    "\n",
    "The inverse document frequency is computed by dividing the total number of documents in the corpus by the number of documents containing the word, and then taking the logarithm of that quotient, i.e.,\n",
    "$$ IDF_t\n",
    " = \\log\\left(\\frac{\\mbox{total number of documents in the corpus}}{\\mbox{number of documents containing the word}}\\right)\n",
    " = \\log\\left(\\frac{N}{|\\{d\\in D: t\\in d\\}|}\\right)\n",
    " $$\n",
    " where $N$ is the total number of documents in the corpus $D$,  and $|\\{d\\in D: t\\in d\\}|$ is the number of documents containing the $t$-word.\n",
    "\n",
    "The IDF value ranges from 0 to $\\infty$. The closer the value is to 0, the more common the word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_idf(corpus):\n",
    "    \"\"\"\n",
    "    Compute the inverse document frequency for each word\n",
    "    :param corpus:\n",
    "    :return: ifd as a pandas series\n",
    "    \"\"\"\n",
    "    # fit and transform the vectorizer to the corpus\n",
    "    vectorizer = CountVectorizer(binary=True) # use binary=True to indicate the presence or absence of a word\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # convert the sparse matrix to a pandas dataframe\n",
    "    bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # compute the inverse document frequency for each word\n",
    "    return np.log(len(corpus) / bow.sum(axis=0))\n",
    "\n",
    "compute_idf(df['review']).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "The term frequency-inverse document frequency (TF-IDF) is the product of the term frequency and the inverse document frequency. The TF-IDF is used to measure how important a word is to a document in a collection of documents (i.e. a corpus). **The higher the TF-IDF, the more important the word is to that specific document**, i.e., relevant words in the document are expected to:\n",
    "- have a high term frequency (i.e. the word appears many times in the document)\n",
    "- have a high inverse-document frequency (i.e. the word appears in a small number of documents in the corpus)\n",
    "\n",
    "\n",
    "The **TF-IDF of a word in a document** is computed as follows:\n",
    "$$\n",
    "\\mbox{TF-IDF}_{t,d}\n",
    "    = \\mbox{term frequency}_{t,d} \\times \\mbox{inverse document frequency}_{t}\n",
    "    = \\frac{n_{t,d}}{\\sum_w n_{w,d}} \\times \\log\\left(\\frac{N}{|\\{d\\in D: t\\in d\\}|}\\right).\n",
    "$$\n",
    "\n",
    "For example, if the corpus has two sentence:\n",
    "- \"The fish is brown\"\n",
    "- \"The fish is green\"\n",
    "\n",
    "then the TF-IDF of the word 'fish' in the first sentence is: 0.25 * log(2/2) = 0.25 * 0 = 0 since\n",
    "- the term frequency of the word 'fish' in the first sentence is 0.25\n",
    "- the inverse document frequency of the word 'fish' is 0 since the word 'fish' appears in all the documents in the corpus.\n",
    "\n",
    "On the other hand, the TF-IDF of the word 'brown' in the first sentence is: 0.25 * log(2/1) = 0.25 * 0.693147 = 0.1733 since\n",
    "- the term frequency of the word 'brown' in the first sentence is 0.25\n",
    "- the inverse document frequency of the word 'brown' is 0.69 since the word 'brown' appears in only one document in the corpus.\n",
    "\n",
    "The TF-IDF ranges from 0 to $\\infty$. The higher the TF-IDF, the more important the word is to that document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the TF-IDF for some review\n",
    "def compute_tfidf(review, idf):\n",
    "    \"\"\" Compute the TF-IDF for a review\n",
    "    :param review: the review\n",
    "    :param idf: the inverse document frequency\n",
    "    :return: the TF-IDF as a pandas series\n",
    "    \"\"\"\n",
    "    # compute the term frequency\n",
    "    tf = compute_term_frequency(review)\n",
    "\n",
    "    # compute the TF-IDF\n",
    "    return tf.mul(idf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'The fish is brown',\n",
    "    'The fish was green',\n",
    "]\n",
    "\n",
    "# compute the TF-IDF for the first sentence\n",
    "compute_tfidf(corpus[0], compute_idf(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tfidf(corpus[1], compute_idf(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_idf(df['review'])\n",
    "compute_tfidf(df['review'][0], idf).T.dropna().sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember\n",
    "\n",
    "    $$ \\mbox{TF-IDF}_{t,d}\n",
    "    = \\mbox{term frequency}_{t,d} \\times \\mbox{inverse document frequency}_{t}\n",
    "    = \\frac{n_{t,d}}{\\sum_w n_{w,d}} \\times \\log\\left(\\frac{N}{|\\{d\\in D: t\\in d\\}|}\\right).\n",
    "    $$\n",
    "\n",
    "So, a **high weight in TF-IDF** is reached by:\n",
    "- a high term frequency (in the given document) and\n",
    "- a low document frequency of the term in the whole collection of documents;\n",
    "\n",
    "The **weights hence tend to filter out common terms**.\n",
    "\n",
    "Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf–idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf–idf closer to 0.\n",
    "\n",
    "(From Wikipedia:)\n",
    "- TF_IDF \"measures\" how important a word is to a document in a collection or corpus.\n",
    "- It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
    "- The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "- tf–idf has been one of the most popular term-weighting schemes. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing entities\n",
    "Recognizing entities is the process of identifying and classifying named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Let us use spaCy to recognize entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load the model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# create a doc object\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# print the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(pd.read_csv('./data/imdb_data_train.zip')['review'].head(10)):\n",
    "    print()\n",
    "    print(doc)\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n",
    "\n",
    "For example, consider the following two sentences:\n",
    "- The cat sat on the mat.\n",
    "- The dog sat on the mat.\n",
    "\n",
    "In this example we have a vocabulary of 5 words. The sentences are 5 words long. We can represent each word using a one-hot encoding, i.e.:\n",
    "\n",
    "    cat = [1, 0, 0, 0, 0]\n",
    "    dog = [0, 1, 0, 0, 0]\n",
    "    mat = [0, 0, 1, 0, 0]\n",
    "    on = [0, 0, 0, 1, 0]\n",
    "    the = [0, 0, 0, 0, 1]\n",
    "\n",
    "We can then represent each sentence as a collection of vectors:\n",
    "\n",
    "        The cat sat on the mat = [[0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
    "        The dog sat on the mat = [[0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud\n",
    "A word cloud is a visualization technique for text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color. This format is useful for quickly perceiving the most prominent terms and for locating a term alphabetically to determine its relative prominence. When used as website navigation aids, the terms are hyperlinked to items associated with the tag.\n",
    "\n",
    "Let us use the wordcloud library to create a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a word cloud\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                      stopwords=STOPWORDS,\n",
    "                      max_words=100,\n",
    "                      random_state=42\n",
    "                      ).generate(' '.join(df['review'].to_list()))\n",
    "\n",
    "# display the word cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "https://monkeylearn.com/sentiment-analysis/\n",
    "\n",
    "### What is it?\n",
    " - **Sentiment analysis** is a technique that uses natural language processing to **analyze the emotions in a piece of text**. It is also known as **opinion mining**, deriving the opinion or attitude of a speaker.\n",
    "\n",
    "- It can be used to analyze social media comments, product reviews, survey responses, and so much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of sentiment analysis\n",
    "There are two main types of sentiment analysis:\n",
    "- **Polarity detection**: Polarity detection is the most common type of sentiment analysis. It involves classifying a statement as either **positive, negative, or neutral**.\n",
    "- **Emotion detection**: Emotion detection is a more advanced type of sentiment analysis that detects emotions in a text. It involves detecting a whole range of emotions, such as, joy, anger, disgust, sadness, fear, surprise, or anticipation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis using text classification\n",
    "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in natural language processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n",
    "\n",
    "The sentiment analysis process using text classification consists of the following steps:\n",
    "- **Data collection**: The first step is to collect the data. This data can be in the form of text, audio, or video. For example, if you want to analyze the sentiment of tweets, you’ll need to collect tweets that you want to analyze.\n",
    "- **Data labeling**: The next step is to label the data. This means that you need to manually assign a sentiment label to each piece of text. For example, if you want to analyze the sentiment of tweets, you’ll need to label each tweet as positive, negative, or neutral. Or with an emotion.\n",
    "- **Preprocessing the data**: The next step is to preprocess the data. This means that you need to clean the data and transform it into a format that can be used by a machine learning algorithm. For example, you can remove punctuation and convert all letters to lowercase.\n",
    "- **Training a text classification model**: The next step is to train a text classification model. This means that you need to feed the labeled data into a machine learning algorithm so that it can learn how to classify text. For example, you can train a text classification model to classify tweets as positive, negative, or neutral.\n",
    "- **Evaluating the model**: The final step is to evaluate the model. This means that you need to test the model on a set of data that it hasn’t seen before. For example, you can test the model on a set of tweets that it hasn’t seen before to see how well it can classify them.\n",
    "- **Deploying the model**: The final step is to deploy the model. This means that you need to make the model available for use. For example, you can deploy the model as a web service so that it can be used to analyze the sentiment of tweets.\n",
    "\n",
    "![text classification](text_processing_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis using BoW\n",
    "\n",
    "Ley us start by remembering our dataset. We have the IMDB dataset of review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_IMDB('./data/imdb_data_train.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For which the distribution of sentiment is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='sentiment').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate the BoW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, # it should already be in lower case...\n",
    "                                   stop_words='english', # stop words should already have been removed but ...\n",
    "                                   ngram_range = (1, 1))\n",
    "\n",
    "cv.fit(df['review'])\n",
    "count_vectors_train = cv.transform(df['review'])\n",
    "count_vectors_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dataframe with BoW and add the sentiment column (for an easier visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train = pd.DataFrame(count_vectors_train.toarray(), columns=cv.get_feature_names_out())\n",
    "bow_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we don't need to split the data into train and test, because that is already provided by the dataset.\n",
    "\n",
    "Let us just try is using a simple decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# this can take a while... +1h on M1\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=20,\n",
    ")\n",
    "tree.fit(bow_train, df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the test data and pass it through the BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_and_preprocess_IMDB('./data/imdb_data_test.zip')\n",
    "count_vectors_test = cv.transform(df_test['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the BoW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = pd.DataFrame(count_vectors_test.toarray(), columns=cv.get_feature_names_out())\n",
    "bow_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And predict the sentiment and coresponding score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(bow_test, df_test['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(idx):\n",
    "    review = df_test.loc[idx, 'original_review']\n",
    "    target = df_test.loc[idx, 'sentiment']\n",
    "\n",
    "    classification = df_test.loc[idx, 'classification']\n",
    "    pred = tree.predict([bow_test.loc[idx]])\n",
    "    pred_proba = tree.predict_proba([bow_test.loc[idx]])\n",
    "\n",
    "    print(f'{\"OK\" if target == pred else \"!OK\"} / real sentiment: {target} (classfication: {classification}) / precicted sentiment: {pred} / pred_proba: {pred_proba}')\n",
    "    print(f'[{idx}]', review)\n",
    "\n",
    "import random\n",
    "\n",
    "for idx in random.sample(range(len(df_test)), 5):\n",
    "    info(idx)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'tree_model.sav'\n",
    "pickle.dump(tree, open(filename, 'wb'))\n",
    "\n",
    "# save the vectorizer to disk\n",
    "filename = 'count_vectorizer.sav'\n",
    "pickle.dump(cv, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Aggarwal, C. (2015). Data Mining: The Textbook. Springer.\n",
    "2. Navlani, A., Fandango, A., & Idris, I. (2021). Python Data Analysis: Perform Data Collection. In Data Processing, Wrangling, Visualization, and Model Building Using Python. Packt Publishing Ltd..\n",
    "2. Zong, C., Xia, R., & Zhang, J. (2021). Text data mining (Vol. 711, p. 712). Singapore: Springer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
